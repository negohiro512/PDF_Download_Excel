import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
import json
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import datetime
import gc  # ãƒ¡ãƒ¢ãƒªè§£æ”¾ç”¨

# --- ç”»é¢è¨­å®š ---
st.set_page_config(page_title="PDFä¸€æ‹¬DL & AIæŠ½å‡º", layout="wide")

st.title("ğŸ“„ PDFä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ & AIå°å¸³ä½œæˆï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰")
st.markdown("""
æŒ‡å®šURLã‹ã‚‰PDFã‚’åé›†ã—ã€**å‰å¹´åº¦å®Ÿç¸¾ï¼ˆå ±å‘Šæ›¸æƒ…å ±ï¼‰**ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
**ã€Œå…¨è‡ªå‹•å®Ÿè¡Œã€**ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€å®Œäº†ã™ã‚‹ã¾ã§è‡ªå‹•ã§åˆ†å‰²å‡¦ç†ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰ã‚’ç¶™ç¶šã—ã¾ã™ã€‚
â€»å‡¦ç†ä¸­ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ãªã„ã§ãã ã•ã„ã€‚
""")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'processed_urls' not in st.session_state:
    st.session_state['processed_urls'] = set()
if 'is_running' not in st.session_state:
    st.session_state['is_running'] = False # å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("ğŸ”‘ APIã‚­ãƒ¼ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        api_key = st.text_input("Gemini APIã‚­ãƒ¼", type="password", help="Google AI Studioã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    st.markdown("---")
    st.subheader("å‡¦ç†è¨­å®š")
    batch_size = st.number_input(
        "1å›ã®å‡¦ç†å˜ä½ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºï¼‰", 
        min_value=1, 
        value=50, 
        step=10, 
        help="ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’é˜²ããŸã‚ã€50ä»¶ç¨‹åº¦ã”ã¨ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾ã‚’è¡Œã„ã¾ã™ã€‚"
    )

    st.markdown("---")
    # å¼·åˆ¶åœæ­¢ãƒœã‚¿ãƒ³
    if st.session_state['is_running']:
        if st.button("ğŸ›‘ å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹"):
            st.session_state['is_running'] = False
            st.warning("ä¸­æ–­å‘½ä»¤ã‚’å‡ºã—ã¾ã—ãŸã€‚ç¾åœ¨ã®ãƒãƒƒãƒãŒçµ‚ã‚ã‚Šæ¬¡ç¬¬åœæ­¢ã—ã¾ã™ã€‚")

    if st.button("ğŸ—‘ï¸ å±¥æ­´ã¨è¨˜æ†¶ã‚’å…¨ã‚¯ãƒªã‚¢"):
        st.session_state['history'] = []
        st.session_state['processed_urls'] = set()
        st.session_state['is_running'] = False
        st.rerun()

    if api_key:
        genai.configure(api_key=api_key)
    st.info("â€»APIã‚­ãƒ¼ãŒãªã„å ´åˆã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æ¬„ ---
col1, col2 = st.columns([2, 1])
with col1:
    default_url = "https://www.pref.kagoshima.jp/aq21/kurashi-kankyo/kankyo/sangyo/seibi/r6_public.html"
    target_url = st.text_input("å¯¾è±¡ã®URL", default_url)
with col2:
    keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—", "06")

# --- é–¢æ•°ç¾¤ ---
def get_pdf_links(target_url, keyword):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    }
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.raise_for_status()
    except Exception as e:
        st.error(f"ã‚µã‚¤ãƒˆã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
    
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.content, "html.parser")
    links = soup.find_all("a")
    
    target_urls = []
    for link in links:
        href = link.get("href")
        if href and href.lower().endswith(".pdf"):
            full_url = urllib.parse.urljoin(target_url, href)
            filename = os.path.basename(urllib.parse.urlparse(full_url).path)
            try:
                filename = urllib.parse.unquote(filename)
            except:
                pass
            
            if not keyword or keyword in filename:
                target_urls.append((filename, full_url))
                
    return list(set(target_urls))

def extract_data_with_ai(pdf_path, filename):
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
    except:
        model = genai.GenerativeModel('gemini-flash-latest')

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        sample_file = genai.upload_file(path=pdf_path, display_name=filename)
        # å¾…æ©Ÿ
        timeout_counter = 0
        while sample_file.state.name == "PROCESSING":
            time.sleep(1)
            timeout_counter += 1
            sample_file = genai.get_file(sample_file.name)
            if timeout_counter > 30: # 30ç§’ä»¥ä¸Šã‹ã‹ã£ãŸã‚‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                return []
        
        if sample_file.state.name == "FAILED":
            return []
    except Exception:
        return []

    prompt = """
    ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®å°‚é–€å®¶ã§ã™ã€‚PDFã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºãƒ»è»¢è¨˜ã—ã¦ãã ã•ã„ã€‚

    ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    è¡¨ã«ã¯ã€Œâ‘ ç¾çŠ¶ï¼ˆå‰å¹´åº¦å®Ÿç¸¾ï¼‰ã€ã¨ã€Œâ‘¡è¨ˆç”»ï¼ˆç›®æ¨™ï¼‰ã€ã®2ã¤ã®åˆ—ãŒä¸¦ã‚“ã§ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    **å¿…ãšã€Œâ‘ ç¾çŠ¶ã€ã¾ãŸã¯ã€Œã€å‰å¹´åº¦å®Ÿç¸¾ã€‘ã€ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹åˆ—ã®æ•°å€¤ã®ã¿**ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    ã€Œâ‘¡è¨ˆç”»ã€ã‚„ã€Œã€ç›®æ¨™ã€‘ã€ã®åˆ—ã®æ•°å€¤ã¯çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã§ãã ã•ã„ã€‚

    ã€æŠ½å‡ºé …ç›®å®šç¾©ã€‘
    1. **æå‡ºæ—¥**: è¡¨ç´™ã®å³ä¸Šã«ã‚ã‚‹æ—¥ä»˜ï¼ˆä¾‹ï¼šä»¤å’Œ6å¹´5æœˆ21æ—¥ï¼‰ã€‚
    2. **å¯¾è±¡å¹´åº¦**: ã€Œâ‘ ç¾çŠ¶ã€ã‚„ã€Œå®Ÿç¸¾ã€ãŒæŒ‡ã—ã¦ã„ã‚‹å¹´åº¦ã€‚
    3. **æ–‡æ›¸ç¨®é¡**: å…¨ã¦ã€Œå ±å‘Šæ›¸ã€ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    4. **äº‹æ¥­ã®ç¨®é¡**: ã€Œäº‹æ¥­ã®ç¨®é¡ã€æ¬„ã‹ã‚‰æŠ½å‡ºã€‚
    5. **äº‹æ¥­å ´å**: ã€Œäº‹æ¥­å ´ã®åç§°ã€ã¾ãŸã¯ã€Œå·¥å ´åãƒ»äº‹æ¥­æ‰€åã€ã‚’æŠ½å‡ºã€‚
    6. **ä½æ‰€**: ã€Œäº‹æ¥­å ´ã®æ‰€åœ¨åœ°ã€ã‚’æŠ½å‡ºã€‚
    7. **å»ƒæ£„ç‰©ã®ç¨®é¡ã”ã¨ã®è¡Œä½œæˆ**: ç”£æ¥­å»ƒæ£„ç‰©ã®ç¨®é¡ã”ã¨ã«1è¡Œä½œæˆã€‚åˆè¨ˆè¡Œã¯ä¸è¦ã€‚

    ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
    JSONå½¢å¼ã®ãƒªã‚¹ãƒˆï¼ˆé…åˆ—ï¼‰ã®ã¿å‡ºåŠ›ã€‚
    
    [
      {
        "æå‡ºæ—¥": "ä»¤å’Œ6å¹´5æœˆ21æ—¥",
        "å¯¾è±¡å¹´åº¦": "ä»¤å’Œ5å¹´åº¦",
        "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸",
        "æ’å‡ºäº‹æ¥­è€…å": "æ ªå¼ä¼šç¤¾ã€‡ã€‡",
        "äº‹æ¥­ã®ç¨®é¡": "ç·åˆå·¥äº‹æ¥­",
        "äº‹æ¥­å ´å": "ç¦å²¡æ”¯åº—",
        "ä½æ‰€": "ç¦å²¡å¸‚åšå¤šåŒº...",
        "å»ƒæ£„ç‰©ã®ç¨®é¡": "ãŒã‚Œãé¡",
        "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": 1299.99,
        "â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 1299.99,
        "â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "è‡ªæ²»ä½“å": "ç¦å²¡å¸‚",
        "å‚™è€ƒ": ""
      }
    ]
    """
    
    try:
        # ç”Ÿæˆå®Ÿè¡Œ
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content([sample_file, prompt], generation_config={"response_mime_type": "application/json"})
        except Exception:
            model = genai.GenerativeModel('gemini-flash-latest')
            response = model.generate_content([sample_file, prompt], generation_config={"response_mime_type": "application/json"})
        
        data_list = json.loads(response.text)
        for item in data_list:
            item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
        return data_list

    except Exception:
        return []

def convert_df_to_excel(df):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, index=False)
        with open(tmp.name, "rb") as f:
            data = f.read()
    return data

# --- äº‹å‰æƒ…å ±å–å¾—ã‚¨ãƒªã‚¢ ---
st.markdown("---")
st.subheader("ğŸ“Š å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")

if target_url:
    # ãƒªãƒ³ã‚¯å…¨å–å¾—
    all_pdf_links = get_pdf_links(target_url, keyword)
    total_count = len(all_pdf_links)
    
    # å‡¦ç†æ¸ˆã¿è¨ˆç®—
    processed_set = st.session_state['processed_urls']
    unprocessed_links = [link for link in all_pdf_links if link[1] not in processed_set]
    remaining_count = len(unprocessed_links)
    processed_count = total_count - remaining_count
    
    # ç”»é¢è¡¨ç¤º
    col_a, col_b, col_c = st.columns(3)
    col_a.metric("å¯¾è±¡PDFç·æ•°", f"{total_count} ä»¶")
    col_b.metric("å®Œäº†", f"{processed_count} ä»¶")
    col_c.metric("æ®‹ã‚Š", f"{remaining_count} ä»¶")
    
    # å…¨ä½“é€²æ—ãƒãƒ¼
    overall_progress = st.progress(0)
    if total_count > 0:
        overall_progress.progress(processed_count / total_count)
    
    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if remaining_count > 0:
        if not st.session_state['is_running']:
            if st.button("ğŸš€ å…¨è‡ªå‹•å®Ÿè¡Œã‚’é–‹å§‹ã™ã‚‹", type="primary"):
                if not api_key:
                    st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
                else:
                    st.session_state['is_running'] = True
                    st.rerun()
    else:
        st.success("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã¾ã™ï¼")

# --- è‡ªå‹•ãƒ«ãƒ¼ãƒ—å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ ---
if st.session_state['is_running']:
    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
    status_box = st.empty()
    batch_progress = st.progress(0)
    
    while remaining_count > 0:
        # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
        if not st.session_state['is_running']:
            status_box.warning("å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            break

        # ä»Šå›ã®ãƒãƒƒãƒã‚’ä½œæˆ
        next_batch = unprocessed_links[:int(batch_size)]
        
        status_box.info(f"ğŸ”„ è‡ªå‹•å‡¦ç†ä¸­... æ®‹ã‚Š {remaining_count} ä»¶ä¸­ã€ä»Šå›ã®ãƒãƒƒãƒ {len(next_batch)} ä»¶ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        # --- ãƒãƒƒãƒå‡¦ç†é–‹å§‹ ---
        with tempfile.TemporaryDirectory() as temp_dir:
            save_dir = os.path.join(temp_dir, "pdfs")
            os.makedirs(save_dir, exist_ok=True)
            
            downloaded_files = []
            headers = {"User-Agent": "Mozilla/5.0"}
            
            # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            for i, (fname, furl) in enumerate(next_batch):
                try:
                    res = requests.get(furl, headers=headers, timeout=10)
                    fpath = os.path.join(save_dir, fname)
                    with open(fpath, "wb") as f:
                        f.write(res.content)
                    downloaded_files.append(fpath)
                    st.session_state['processed_urls'].add(furl) # å‡¦ç†æ¸ˆã¿ã«ç™»éŒ²
                except Exception:
                    pass # ã‚¨ãƒ©ãƒ¼ã§ã‚‚æ­¢ã¾ã‚‰ãšæ¬¡ã¸
                
                # ãƒãƒƒãƒå†…é€²æ—æ›´æ–°
                batch_progress.progress((i + 1) / len(next_batch) * 0.5) # å‰åŠ50%
            
            # 2. AIè§£æ
            if downloaded_files:
                batch_data = []
                for i, fpath in enumerate(downloaded_files):
                    fname = os.path.basename(fpath)
                    extracted = extract_data_with_ai(fpath, fname)
                    if extracted:
                        batch_data.extend(extracted)
                    
                    # ãƒãƒƒãƒå†…é€²æ—æ›´æ–°
                    batch_progress.progress(0.5 + (i + 1) / len(downloaded_files) * 0.5) # å¾ŒåŠ50%
                
                # 3. çµæœä¿å­˜
                if batch_data:
                    df = pd.DataFrame(batch_data)
                    # åˆ—æ•´ç†
                    column_mapping = {
                        'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                        'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                        'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                        'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                        'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                        'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                        'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                        'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                        'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                        'å‚™è€ƒ': 'å‚™è€ƒ'
                    }
                    target_cols = [c for c in column_mapping.keys() if c in df.columns]
                    df = df[target_cols].rename(columns=column_mapping)
                    
                    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    st.session_state['history'].append({
                        "time": now,
                        "keyword": keyword,
                        "count": len(df),
                        "df": df
                    })
        
        # --- ãƒ¡ãƒ¢ãƒªè§£æ”¾ ---
        del downloaded_files
        del batch_data
        gc.collect()
        
        # æ®‹ã‚Šä»¶æ•°ã‚’å†è¨ˆç®—
        unprocessed_links = [link for link in all_pdf_links if link[1] not in st.session_state['processed_urls']]
        remaining_count = len(unprocessed_links)
        
        # å…¨ä½“é€²æ—ãƒãƒ¼æ›´æ–°
        processed_count = total_count - remaining_count
        if total_count > 0:
            overall_progress.progress(processed_count / total_count)
            
        # å®Œäº†ãƒã‚§ãƒƒã‚¯
        if remaining_count == 0:
            st.session_state['is_running'] = False
            status_box.success("ğŸ‰ å…¨ä»¶ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            st.rerun()
            break
        else:
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã¸
            time.sleep(1)

# --- å®Ÿè¡Œå±¥æ­´ã‚¨ãƒªã‚¢ ---
st.markdown("---")
st.subheader("ğŸ“‚ å®Ÿè¡Œå±¥æ­´ & çµ±åˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

if len(st.session_state['history']) > 0:
    all_dfs = [item['df'] for item in st.session_state['history']]
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    st.info(f"ç¾åœ¨ã€åˆè¨ˆ **{len(merged_df)} è¡Œ** ã®ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã™ã€‚")
    
    merged_excel = convert_df_to_excel(merged_df)
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    st.download_button(
        label="ğŸ“¦ ã™ã¹ã¦ã®çµæœã‚’çµåˆã—ã¦Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=merged_excel,
        file_name=f"waste_report_TOTAL_{now_str}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key="download_total_btn",
        type="primary"
    )
    
    with st.expander("å€‹åˆ¥ã®å±¥æ­´ã‚’è¦‹ã‚‹"):
        for i, item in enumerate(reversed(st.session_state['history'])):
            st.write(f"**{item['time']}** - {item['count']}ä»¶")
            st.dataframe(item['df'])
else:
    st.write("å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
