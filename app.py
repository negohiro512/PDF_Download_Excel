import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
import json
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai

# --- ç”»é¢è¨­å®š ---
st.set_page_config(page_title="PDFä¸€æ‹¬DL & AIæŠ½å‡º", layout="wide")

st.title("ğŸ“„ PDFä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ & AIå°å¸³ä½œæˆ")
st.markdown("""
æŒ‡å®šURLã‹ã‚‰PDFã‚’åé›†ã—ã€**å‰å¹´åº¦å®Ÿç¸¾ï¼ˆå ±å‘Šæ›¸æƒ…å ±ï¼‰ã®ã¿**ã‚’æŠ½å‡ºã—ã¦ExcelåŒ–ã—ã¾ã™ã€‚
è¨ˆç”»å€¤ï¼ˆç›®æ¨™ï¼‰ã¯é™¤å¤–ã•ã‚Œã¾ã™ã€‚
""")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    api_key = st.text_input("Gemini APIã‚­ãƒ¼", type="password", help="Google AI Studioã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if api_key:
        genai.configure(api_key=api_key)
    st.info("â€»APIã‚­ãƒ¼ãŒãªã„å ´åˆã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æ¬„ ---
col1, col2 = st.columns([2, 1])
with col1:
    default_url = "https://www.city.fukuoka.lg.jp/kankyo/sanhai/hp/sangyouhaikibutu/haisyutujigyousya/taryoukouhyou.html"
    target_url = st.text_input("å¯¾è±¡ã®URL", default_url)
with col2:
    keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—", "06")

# --- é–¢æ•°ï¼šPDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
def download_pdfs(target_url, keyword, save_dir, status_text, progress_bar):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": "https://www.google.com/"
    }
    
    status_text.text("ã‚µã‚¤ãƒˆã®æƒ…å ±ã‚’å–å¾—ä¸­...")
    try:
        response = requests.get(target_url, headers=headers, timeout=10)
        response.raise_for_status()
    except Exception as e:
        st.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.content, "html.parser")
    links = soup.find_all("a")
    
    download_targets = []
    for link in links:
        href = link.get("href")
        if href and href.lower().endswith(".pdf"):
            full_url = urllib.parse.urljoin(target_url, href)
            filename = os.path.basename(urllib.parse.urlparse(full_url).path)
            try:
                filename = urllib.parse.unquote(filename)
            except:
                pass
            
            if not keyword or keyword in filename:
                download_targets.append((filename, full_url))
    
    download_targets = list(set(download_targets))
    if not download_targets:
        return []
    
    downloaded_files = []
    status_text.text(f"{len(download_targets)} ä»¶ã®PDFãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    for i, (filename, url) in enumerate(download_targets):
        try:
            file_res = requests.get(url, headers=headers, timeout=10)
            file_path = os.path.join(save_dir, filename)
            with open(file_path, "wb") as f:
                f.write(file_res.content)
            downloaded_files.append(file_path)
            progress_bar.progress((i + 1) / len(download_targets))
            time.sleep(1)
        except Exception as e:
            st.warning(f"{filename} ã®å–å¾—å¤±æ•—: {e}")
            
    return downloaded_files

# --- é–¢æ•°ï¼šAIã«ã‚ˆã‚‹æŠ½å‡ºï¼ˆå®Ÿç¸¾ã®ã¿ã«é™å®šï¼‰ ---
def extract_data_with_ai(pdf_path, filename):
    # Gemini 2.5 Flash (Experimental) ã‚’å„ªå…ˆ
    try:
        model = genai.GenerativeModel('gemini-2.5-flash-exp')
    except:
        model = genai.GenerativeModel('gemini-2.5-flash')
    
    try:
        sample_file = genai.upload_file(path=pdf_path, display_name=filename)
        while sample_file.state.name == "PROCESSING":
            time.sleep(1)
            sample_file = genai.get_file(sample_file.name)
        if sample_file.state.name == "FAILED":
            return []
    except Exception as e:
        st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡ç¤ºæ›¸ï¼‰ï¼šå®Ÿç¸¾ã®ã¿ã«é™å®š
    prompt = """
    ã“ã®PDFã¯ç”£æ¥­å»ƒæ£„ç‰©ã®å‡¦ç†è¨ˆç”»æ›¸ãƒ»å ±å‘Šæ›¸ã§ã™ã€‚
    PDFå†…ã®è¡¨ï¼ˆç‰¹ã«åˆ¥ç´™ã®å†…è¨³è¡¨ï¼‰ã‹ã‚‰ã€**ã€Œå‰å¹´åº¦å®Ÿç¸¾ï¼ˆç¾çŠ¶ï¼‰ã€**ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    
    ã€é‡è¦ï¼šæŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
    1. **å®Ÿç¸¾ã®ã¿æŠ½å‡º**: ã€Œè¨ˆç”»ã€ã‚„ã€Œç›®æ¨™ã€ã®æ•°å€¤ã¯**å…¨ã¦ç„¡è¦–**ã—ã¦ãã ã•ã„ã€‚æŠ½å‡ºå¯¾è±¡ã¯ã€Œå®Ÿç¸¾ã€ã‚„ã€Œç¾çŠ¶ã€ã¨æ›¸ã‹ã‚ŒãŸæ¬„ã®æ•°å€¤ã®ã¿ã§ã™ã€‚
    2. **å¯¾è±¡å¹´åº¦**: å®Ÿç¸¾å€¤ã®å¯¾è±¡ã¨ãªã£ã¦ã„ã‚‹å¹´åº¦ï¼ˆä¾‹ï¼šæå‡ºæ—¥ãŒä»¤å’Œ6å¹´5æœˆãªã‚‰ã€å¯¾è±¡å¹´åº¦ã¯ã€Œä»¤å’Œ5å¹´åº¦ã€ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    3. **ç¨®é¡ã”ã¨ã®åˆ†å‰²**: åˆè¨ˆè¡Œã§ã¯ãªãã€å»ƒæ£„ç‰©ã®ç¨®é¡ã”ã¨ã«1è¡Œãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    4. **æ–‡æ›¸ç¨®é¡**: å…¨ã¦ã€Œå ±å‘Šæ›¸ã€ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆå®Ÿç¸¾å€¤ã‚’æ‰±ã†ãŸã‚ï¼‰ã€‚
    5. **æå‡ºæ—¥**: è¡¨ç´™ã®æå‡ºæ—¥ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    ä»¥ä¸‹ã®JSONå½¢å¼ã®ãƒªã‚¹ãƒˆï¼ˆé…åˆ—ï¼‰ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è©²å½“ã™ã‚‹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    
    [
      {
        "æå‡ºæ—¥": "ä»¤å’Œ6å¹´5æœˆ21æ—¥",
        "å¯¾è±¡å¹´åº¦": "ä»¤å’Œ5å¹´åº¦",
        "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸",
        "æ’å‡ºäº‹æ¥­è€…å": "æ ªå¼ä¼šç¤¾ã€‡ã€‡",
        "å»ƒæ£„ç‰©ã®ç¨®é¡": "ãŒã‚Œãé¡",
        "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": 100.5,
        "â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 100.5,
        "â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "è‡ªæ²»ä½“å": "ç¦å²¡å¸‚",
        "å‚™è€ƒ": ""
      }
    ]
    """
    
    try:
        response = model.generate_content(
            [sample_file, prompt],
            generation_config={"response_mime_type": "application/json"}
        )
        data_list = json.loads(response.text)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å„ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        for item in data_list:
            item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
            
        return data_list
    except Exception as e:
        return []

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if st.button("ğŸš€ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹"):
    if not api_key:
        st.error("AIæŠ½å‡ºã‚’è¡Œã†ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        status_text = st.empty()
        progress_bar = st.progress(0)

        with tempfile.TemporaryDirectory() as temp_dir:
            save_dir = os.path.join(temp_dir, "pdfs")
            os.makedirs(save_dir, exist_ok=True)
            
            # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            downloaded_files = download_pdfs(target_url, keyword, save_dir, status_text, progress_bar)
            
            if not downloaded_files:
                st.warning("æ¡ä»¶ã«åˆã†PDFãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                status_text.text("AIã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")
                progress_bar.progress(0)
                
                all_extracted_data = []
                
                # 2. AIæŠ½å‡ºãƒ«ãƒ¼ãƒ—
                for i, pdf_path in enumerate(downloaded_files):
                    filename = os.path.basename(pdf_path)
                    status_text.text(f"åˆ†æä¸­ ({i+1}/{len(downloaded_files)}): {filename}")
                    
                    extracted_list = extract_data_with_ai(pdf_path, filename)
                    if extracted_list:
                        all_extracted_data.extend(extracted_list)
                    
                    progress_bar.progress((i + 1) / len(downloaded_files))
                
                # 3. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨ExcelåŒ–
                if all_extracted_data:
                    df = pd.DataFrame(all_extracted_data)
                    
                    # ã‚«ãƒ©ãƒ é †åºã¨ãƒªãƒãƒ¼ãƒ 
                    column_mapping = {
                        'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å',
                        'æå‡ºæ—¥': 'æå‡ºæ—¥',
                        'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦',
                        'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡',
                        'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å',
                        'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                        'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                        'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                        'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                        'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                        'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                        'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å',
                        'å‚™è€ƒ': 'å‚™è€ƒ'
                    }
                    
                    target_cols = [c for c in column_mapping.keys() if c in df.columns]
                    df = df[target_cols]
                    df = df.rename(columns=column_mapping)

                    st.success(f"ğŸ‰ å‡¦ç†å®Œäº†ï¼ {len(df)} ä»¶ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                    st.dataframe(df)
                    
                    excel_path = os.path.join(temp_dir, "waste_report_results_only.xlsx")
                    df.to_excel(excel_path, index=False)
                    
                    with open(excel_path, "rb") as f:
                        st.download_button(
                            label="ğŸ“¥ å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®Excelã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=f,
                            file_name="waste_report_results_only.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                else:
                    st.error("ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¡ä»¶ã«åˆã†å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
