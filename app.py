import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
import json
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import datetime
import gc  # ãƒ¡ãƒ¢ãƒªè§£æ”¾ç”¨

# --- ç”»é¢è¨­å®š ---
st.set_page_config(page_title="ç”£å»ƒå ±å‘Šæ›¸AIæŠ½å‡ºã‚¢ãƒ—ãƒª", layout="wide")

st.title("ğŸ“„ ç”£å»ƒå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»å°å¸³ä½œæˆã‚¢ãƒ—ãƒª")
st.markdown("""
**ã€ŒWebè‡ªå‹•åé›†ã€** ã¾ãŸã¯ **ã€Œæ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€** ã§ã€å ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ä¸€è¦§åŒ–ã—ã¾ã™ã€‚
**PDFãƒ•ã‚¡ã‚¤ãƒ«** ã¨ **Excelãƒ•ã‚¡ã‚¤ãƒ«** ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
""")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'processed_urls' not in st.session_state:
    st.session_state['processed_urls'] = set()
if 'is_running' not in st.session_state:
    st.session_state['is_running'] = False

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("ğŸ”‘ APIã‚­ãƒ¼ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        api_key = st.text_input("Gemini APIã‚­ãƒ¼", type="password", help="Google AI Studioã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã¨è¨˜æ†¶ã‚’å…¨ã‚¯ãƒªã‚¢"):
        st.session_state['history'] = []
        st.session_state['processed_urls'] = set()
        st.session_state['is_running'] = False
        st.rerun()

    if api_key:
        genai.configure(api_key=api_key)
    st.info("â€»APIã‚­ãƒ¼ãŒãªã„å ´åˆã€å‹•ä½œã—ã¾ã›ã‚“ã€‚")

# ==========================================
# ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼šãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°ç¾¤
# ==========================================

# --- Excelå¼·åŠ›èª­ã¿å–ã‚Šé–¢æ•° (ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºç‰ˆ) ---
def read_excel_robust(file_path):
    extracted_data = []
    try:
        xls = pd.ExcelFile(file_path)
        
        for sheet_name in xls.sheet_names:
            try:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’èª­ã¿è¾¼ã‚€
                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            except Exception as e:
                # ãƒ­ã‚°ã«å‡ºã™ï¼ˆèª­ã¿è¾¼ã‚ãªã„ã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                st.write(f"âš ï¸ Sheetèª­è¾¼ã‚¨ãƒ©ãƒ¼: {sheet_name} -> {e}")
                continue
            
            # --- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¢ç´¢ ---
            target_row_idx = -1
            col_mapping = {} 
            
            for r_idx, row in df.iterrows():
                row_str = row.astype(str).values
                # çµåˆã‚»ãƒ«ãªã©ã®æ±šã‚Œã‚’å–ã£ã¦åˆ¤å®š
                if any("å»ƒæ£„ç‰©ã®ç¨®é¡" in s for s in row_str) or any("ç”£æ¥­å»ƒæ£„ç‰©ã®ç¨®é¡" in s for s in row_str):
                    target_row_idx = r_idx
                    for c_idx, cell_val in enumerate(row_str):
                        val = str(cell_val).replace("\n", "").replace(" ", "")
                        if "ç¨®é¡" in val:
                            col_mapping["kind"] = c_idx
                        elif "å…¨å‡¦ç†å§”è¨—é‡" in val or "å§”è¨—é‡" in val:
                            col_mapping["amount"] = c_idx
                    break 
            
            # ç›®å°ãŒè¦‹ã¤ã‹ã‚Šã€ã‹ã¤å¿…è¦ãªåˆ—ãŒæƒã£ã¦ã„ã‚‹å ´åˆã®ã¿æŠ½å‡º
            if target_row_idx != -1 and "kind" in col_mapping and "amount" in col_mapping:
                start_row = target_row_idx + 1
                for i in range(start_row, len(df)):
                    if col_mapping["kind"] >= len(df.columns) or col_mapping["amount"] >= len(df.columns):
                        continue

                    kind_val = df.iloc[i, col_mapping["kind"]]
                    amount_val = df.iloc[i, col_mapping["amount"]]
                    
                    if pd.notna(kind_val) and pd.notna(amount_val):
                        try:
                            # æ•°å€¤å¤‰æ›ã§ãã‚‹ã‚‚ã®ã ã‘å–å¾—ï¼ˆã€Œåˆè¨ˆã€è¡Œãªã©ã‚’é™¤å¤–ï¼‰
                            amt_str = str(amount_val).replace(",", "").strip()
                            amt = float(amt_str)
                            waste_type = str(kind_val).strip()
                            
                            # ã‚´ãƒŸãƒ‡ãƒ¼ã‚¿ã®æ’é™¤
                            if "åˆè¨ˆ" in waste_type or waste_type == "" or waste_type == "nan":
                                continue

                            extracted_data.append({
                                "æå‡ºæ—¥": "",
                                "å¯¾è±¡å¹´åº¦": "",
                                "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸",
                                "æ’å‡ºäº‹æ¥­è€…å": "",
                                "äº‹æ¥­ã®ç¨®é¡": "",
                                "äº‹æ¥­å ´å": "",
                                "ä½æ‰€": "",
                                "è‡ªæ²»ä½“å": "",
                                "å»ƒæ£„ç‰©ã®ç¨®é¡": waste_type,
                                "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": amt,
                                "å‚™è€ƒ": f"Sheet: {sheet_name}"
                            })
                        except ValueError:
                            continue 

    except Exception as e:
        st.error(f"âŒ Excelãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã®èª­è¾¼å¤±æ•—: {e}")
        return []
        
    return extracted_data


# --- å…±é€šé–¢æ•°ï¼šãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼†ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºç‰ˆï¼‰ ---
def extract_data_with_ai(file_path, filename):
    file_ext = os.path.splitext(filename)[1].lower()
    
    # ãƒ­ã‚°è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠï¼ˆé–‰ã˜ã¦ãŠãï¼‰
    with st.expander(f"ğŸ” è§£æãƒ­ã‚°: {filename}", expanded=False):
        
        # ------------------------------------------------
        # 1. Excelã®å ´åˆ
        # ------------------------------------------------
        if file_ext in [".xlsx", ".xls"]:
            st.write("ğŸ”¹ Pythonè§£æã‚’å®Ÿè¡Œä¸­...")
            data_list = read_excel_robust(file_path)
            
            if len(data_list) > 0:
                st.success(f"âœ… Pythonã§ {len(data_list)} è¡ŒæŠ½å‡ºæˆåŠŸï¼")
                for item in data_list:
                    item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
                    if "æ’å‡ºäº‹æ¥­è€…å" in item and not item["æ’å‡ºäº‹æ¥­è€…å"]:
                        item["æ’å‡ºäº‹æ¥­è€…å"] = filename
                return data_list
            
            st.warning("ğŸ”¸ PythonæŠ½å‡º 0ä»¶ -> AIæ•‘æ¸ˆãƒ¢ãƒ¼ãƒ‰ã¸ç§»è¡Œã—ã¾ã™")
            
            try:
                # Excelãƒ†ã‚­ã‚¹ãƒˆåŒ–
                xls = pd.read_excel(file_path, sheet_name=None)
                text_buffer = f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n\n"
                for sheet_name, df in xls.items():
                    text_buffer += f"--- Sheet: {sheet_name} ---\n"
                    text_buffer += df.fillna("").to_csv(index=False)
                    text_buffer += "\n\n"
                
                # æ–‡å­—æ•°ãŒå¤šã™ãã‚‹å ´åˆã®ã‚¬ãƒ¼ãƒ‰
                if len(text_buffer) > 30000:
                    st.write("âš ï¸ ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ãŸã‚ãƒˆãƒªãƒŸãƒ³ã‚°ã—ã¦é€ä¿¡ã—ã¾ã™")
                    text_buffer = text_buffer[:30000]

                prompt_text = """
                ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®å°‚é–€å®¶ã§ã™ã€‚Excelãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†å ±å‘Šæ›¸ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã€Œå»ƒæ£„ç‰©ã®ç¨®é¡ã€ã¨ã€Œå…¨å‡¦ç†å§”è¨—é‡(å®Ÿç¸¾)ã€ã®ãƒšã‚¢ã‚’å…¨ã¦æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
                åˆè¨ˆè¡Œã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚
                å‡ºåŠ›ã¯JSONå½¢å¼ã®ãƒªã‚¹ãƒˆã®ã¿ã€‚
                [{"å»ƒæ£„ç‰©ã®ç¨®é¡": "xx", "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": 10.5, "å‚™è€ƒ": "AIæŠ½å‡º"}]
                """
                
                st.write("ğŸ”¹ Gemini API å‘¼ã³å‡ºã—ä¸­...")
                try:
                    model = genai.GenerativeModel('gemini-2.5-flash')
                    response = model.generate_content([prompt_text, text_buffer], generation_config={"response_mime_type": "application/json"})
                except Exception as e:
                    st.write(f"  - flashãƒ¢ãƒ‡ãƒ«å¤±æ•—: {e}, latestãƒ¢ãƒ‡ãƒ«ã§å†è©¦è¡Œ...")
                    model = genai.GenerativeModel('gemini-flash-latest')
                    response = model.generate_content([prompt_text, text_buffer], generation_config={"response_mime_type": "application/json"})

                ai_data_list = json.loads(response.text)
                st.success(f"âœ… AIæ•‘æ¸ˆæˆåŠŸ: {len(ai_data_list)} è¡ŒæŠ½å‡º")
                
                for item in ai_data_list:
                    item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
                    # å¿…é ˆé …ç›®ã®è£œå®Œ
                    if "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton" not in item: item["â‘©å…¨å‡¦ç†å§”è¨—é‡_ton"] = 0
                
                return ai_data_list

            except Exception as e:
                st.error(f"âŒ AIè§£æã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return []

        # ------------------------------------------------
        # 2. PDFã®å ´åˆ
        # ------------------------------------------------
        elif file_ext == ".pdf":
            st.write("ğŸ”¹ PDFè§£æ(AI)ã‚’å®Ÿè¡Œä¸­...")
            try:
                try:
                    model = genai.GenerativeModel('gemini-2.5-flash')
                except:
                    model = genai.GenerativeModel('gemini-flash-latest')

                sample_file = genai.upload_file(path=file_path, display_name=filename)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¾…ã¡ãƒ«ãƒ¼ãƒ—
                timeout_counter = 0
                while sample_file.state.name == "PROCESSING":
                    time.sleep(1)
                    timeout_counter += 1
                    sample_file = genai.get_file(sample_file.name)
                    
                    # ã€é‡è¦ä¿®æ­£ã€‘ç”»åƒPDFå¯¾ç­–ï¼šå¾…ã¡æ™‚é–“ã‚’30ç§’â†’600ç§’ã«å»¶é•·
                    if timeout_counter > 600: 
                        st.error("âŒ PDFå‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (10åˆ†çµŒé)")
                        return []
                
                if sample_file.state.name == "FAILED": 
                    st.error("âŒ Googleå´ã§PDFå‡¦ç†å¤±æ•—")
                    return []
                
                prompt_text = """
                ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®å°‚é–€å®¶ã§ã™ã€‚è³‡æ–™ã‹ã‚‰ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                å¿…ãšã€Œâ‘ ç¾çŠ¶ï¼ˆå®Ÿç¸¾ï¼‰ã€ã®æ•°å€¤ã®ã¿ã‚’æŠ½å‡ºã—ã€ã€Œâ‘¡è¨ˆç”»ã€ã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚
                
                ã€å‡ºåŠ›é …ç›®ã€‘
                æå‡ºæ—¥, å¯¾è±¡å¹´åº¦, æ–‡æ›¸ç¨®é¡(å ±å‘Šæ›¸), æ’å‡ºäº‹æ¥­è€…å, äº‹æ¥­ã®ç¨®é¡, äº‹æ¥­å ´å, ä½æ‰€, 
                å»ƒæ£„ç‰©ã®ç¨®é¡, â‘©å…¨å‡¦ç†å§”è¨—é‡_ton, â‘ªå„ªè‰¯èªå®š(t), â‘«å†ç”Ÿåˆ©ç”¨(t), â‘¬ç†±å›åèªå®š(t), â‘­ç†±å›åãã®ä»–(t), è‡ªæ²»ä½“å

                ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
                JSONå½¢å¼ã®ãƒªã‚¹ãƒˆã®ã¿ã€‚
                """

                # ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãå‘¼ã³å‡ºã—
                try:
                    response = model.generate_content([sample_file, prompt_text], generation_config={"response_mime_type": "application/json"})
                except:
                    time.sleep(2)
                    response = model.generate_content([sample_file, prompt_text], generation_config={"response_mime_type": "application/json"})
                
                data_list = json.loads(response.text)
                st.success(f"âœ… PDFè§£ææˆåŠŸ: {len(data_list)} è¡ŒæŠ½å‡º")
                
                for item in data_list:
                    item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
                return data_list

            except Exception as e:
                st.error(f"âŒ PDFè§£æã‚¨ãƒ©ãƒ¼: {e}")
                return []
        
        else:
            st.write(f"âš ï¸ æœªå¯¾å¿œã®æ‹¡å¼µå­: {file_ext}")
            return []

def convert_df_to_excel(df):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, index=False)
        with open(tmp.name, "rb") as f:
            data = f.read()
    return data

# ==========================================
# ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ==========================================
tab1, tab2 = st.tabs(["ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†æ", "ğŸŒ URLã‹ã‚‰è‡ªå‹•åé›†"])

# ------------------------------------------
# ã‚¿ãƒ–1ï¼šæ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
# ------------------------------------------
with tab1:
    st.subheader("æ‰‹æŒã¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ")
    st.write("PDF ã¾ãŸã¯ Excelãƒ•ã‚¡ã‚¤ãƒ«(.xlsx, .xls) ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚")
    
    uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["pdf", "xlsx", "xls"], accept_multiple_files=True)
    
    if uploaded_files:
        st.info(f"{len(uploaded_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        if st.button("ğŸš€ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æé–‹å§‹", type="primary"):
            if not api_key:
                st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            else:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "uploads")
                    os.makedirs(save_dir, exist_ok=True)
                    
                    batch_data = []
                    status_text.text("AIã¨Pythonã«ã‚ˆã‚‹åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
                    
                    for i, uploaded_file in enumerate(uploaded_files):
                        file_path = os.path.join(save_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        status_text.text(f"åˆ†æä¸­ ({i+1}/{len(uploaded_files)}): {uploaded_file.name}")
                        extracted = extract_data_with_ai(file_path, uploaded_file.name)
                        if extracted:
                            batch_data.extend(extracted)
                        
                        progress_bar.progress((i + 1) / len(uploaded_files))
                    
                    if batch_data:
                        df = pd.DataFrame(batch_data)
                        column_mapping = {
                            'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                            'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                            'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                            'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                            'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                            'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                            'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                            'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                            'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                            'å‚™è€ƒ': 'å‚™è€ƒ'
                        }
                        target_cols = [c for c in column_mapping.keys() if c in df.columns]
                        df = df[target_cols].rename(columns=column_mapping)
                        
                        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        st.session_state['history'].append({
                            "time": now,
                            "keyword": "æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                            "count": len(df),
                            "df": df
                        })
                        
                        st.success(f"ğŸ‰ åˆ†æå®Œäº†ï¼ {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚ä¸‹ã®å±¥æ­´ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
                        time.sleep(1)
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    
                    gc.collect()

# ------------------------------------------
# ã‚¿ãƒ–2ï¼šURLè‡ªå‹•åé›†æ©Ÿèƒ½
# ------------------------------------------
with tab2:
    st.subheader("Webã‚µã‚¤ãƒˆã‹ã‚‰è‡ªå‹•åé›†")
    st.write("å¯¾è±¡URLã«ã‚ã‚‹ PDF ãŠã‚ˆã³ Excelãƒ•ã‚¡ã‚¤ãƒ« ã‚’è‡ªå‹•åé›†ã—ã¾ã™ã€‚")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        default_url = "https://www.pref.tokushima.lg.jp/jigyoshanokata/kurashi/recycling/7300999"
        target_url = st.text_input("å¯¾è±¡ã®URL", default_url)
    with col2:
        keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—", "")

    batch_size = st.number_input("è‡ªå‹•å‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚º", min_value=1, value=50, step=10)

    # ãƒªãƒ³ã‚¯å–å¾—é–¢æ•°
    def get_file_links(target_url, keyword):
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(target_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, "html.parser")
            links = soup.find_all("a")
            target_urls = []
            for link in links:
                href = link.get("href")
                if href:
                    href_lower = href.lower()
                    if href_lower.endswith(".pdf") or href_lower.endswith(".xlsx") or href_lower.endswith(".xls"):
                        full_url = urllib.parse.urljoin(target_url, href)
                        filename = os.path.basename(urllib.parse.urlparse(full_url).path)
                        try: filename = urllib.parse.unquote(filename)
                        except: pass
                        
                        if not keyword or keyword in filename:
                            target_urls.append((filename, full_url))
            return list(set(target_urls))
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    if target_url:
        all_file_links = get_file_links(target_url, keyword)
        processed_set = st.session_state['processed_urls']
        unprocessed_links = [link for link in all_file_links if link[1] not in processed_set]
        remaining_count = len(unprocessed_links)
        
        st.caption(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ç·æ•°: {len(all_file_links)}ä»¶ / å®Œäº†: {len(all_file_links)-remaining_count}ä»¶ / æ®‹ã‚Š: {remaining_count}ä»¶")

        if remaining_count > 0:
            if not st.session_state['is_running']:
                if st.button("ğŸš€ URLã‹ã‚‰ã®è‡ªå‹•å®Ÿè¡Œã‚’é–‹å§‹", type="primary"):
                    st.session_state['is_running'] = True
                    st.rerun()
        
        if st.session_state['is_running']:
            status_box = st.empty()
            batch_progress = st.progress(0)
            
            while remaining_count > 0:
                if not st.session_state['is_running']: break
                next_batch = unprocessed_links[:int(batch_size)]
                status_box.info(f"ğŸ”„ è‡ªå‹•å‡¦ç†ä¸­... æ®‹ã‚Š {remaining_count} ä»¶")
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "downloads")
                    os.makedirs(save_dir, exist_ok=True)
                    downloaded_files = []
                    headers = {"User-Agent": "Mozilla/5.0"}
                    
                    for i, (fname, furl) in enumerate(next_batch):
                        try:
                            res = requests.get(furl, headers=headers, timeout=10)
                            fpath = os.path.join(save_dir, fname)
                            with open(fpath, "wb") as f: f.write(res.content)
                            downloaded_files.append(fpath)
                            st.session_state['processed_urls'].add(furl)
                        except: pass
                        batch_progress.progress((i + 1) / len(next_batch) * 0.5)
                    
                    if downloaded_files:
                        batch_data = []
                        for i, fpath in enumerate(downloaded_files):
                            fname = os.path.basename(fpath)
                            extracted = extract_data_with_ai(fpath, fname)
                            if extracted: batch_data.extend(extracted)
                            batch_progress.progress(0.5 + (i + 1) / len(downloaded_files) * 0.5)
                        
                        if batch_data:
                            df = pd.DataFrame(batch_data)
                            column_mapping = {
                                'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                                'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                                'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                                'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                                'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                                'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                                'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                                'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                                'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                                'å‚™è€ƒ': 'å‚™è€ƒ'
                            }
                            target_cols = [c for c in column_mapping.keys() if c in df.columns]
                            df = df[target_cols].rename(columns=column_mapping)
                            
                            now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            st.session_state['history'].append({
                                "time": now, "keyword": keyword, "count": len(df), "df": df
                            })
                
                del downloaded_files
                gc.collect()
                # ãƒªã‚¹ãƒˆæ›´æ–°
                unprocessed_links = [link for link in all_file_links if link[1] not in st.session_state['processed_urls']]
                remaining_count = len(unprocessed_links)
                
                if remaining_count == 0:
                    st.session_state['is_running'] = False
                    status_box.success("å®Œäº†ï¼")
                    st.rerun()
                else:
                    time.sleep(1)

            if st.button("ğŸ›‘ ä¸­æ–­"):
                st.session_state['is_running'] = False
                st.rerun()

# --- å…±é€šï¼šå®Ÿè¡Œå±¥æ­´ã‚¨ãƒªã‚¢ ---
st.markdown("---")
st.subheader("ğŸ“‚ å®Ÿè¡Œå±¥æ­´ & çµ±åˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

if len(st.session_state['history']) > 0:
    all_dfs = [item['df'] for item in st.session_state['history']]
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    st.info(f"ğŸ’¡ ç¾åœ¨åˆè¨ˆ **{len(merged_df)} è¡Œ** ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚")
    
    merged_excel = convert_df_to_excel(merged_df)
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    st.download_button(
        label="ğŸ“¦ ã™ã¹ã¦ã®çµæœã‚’çµåˆã—ã¦Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=merged_excel,
        file_name=f"waste_report_TOTAL_{now_str}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key="download_total_btn",
        type="primary"
    )
    
    with st.expander("å€‹åˆ¥ã®å±¥æ­´ã‚’è¦‹ã‚‹"):
        for i, item in enumerate(reversed(st.session_state['history'])):
            st.write(f"**{item['time']}** - [{item['keyword']}] {item['count']}ä»¶")
            st.dataframe(item['df'])
else:
    st.write("å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
