import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
import json
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import datetime
import gc  # ãƒ¡ãƒ¢ãƒªè§£æ”¾ç”¨
import re   # JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç”¨

# --- ç”»é¢è¨­å®š ---
st.set_page_config(page_title="ç”£å»ƒå ±å‘Šæ›¸AIæŠ½å‡ºã‚¢ãƒ—ãƒª", layout="wide")

st.title("ğŸ“„ ç”£å»ƒå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»å°å¸³ä½œæˆã‚¢ãƒ—ãƒª")
st.markdown("""
**ã€ŒWebè‡ªå‹•åé›†ã€** ã¾ãŸã¯ **ã€Œæ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€** ã§ã€å ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ä¸€è¦§åŒ–ã—ã¾ã™ã€‚
**PDFãƒ•ã‚¡ã‚¤ãƒ«** ã¨ **Excelãƒ•ã‚¡ã‚¤ãƒ«** ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
""")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'processed_urls' not in st.session_state:
    st.session_state['processed_urls'] = set()
if 'is_running' not in st.session_state:
    st.session_state['is_running'] = False
# ç›£æŸ»ç”¨ï¼šWebä¸Šã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿æŒï¼ˆé †åºä¿æŒãƒªã‚¹ãƒˆï¼‰
if 'all_target_files' not in st.session_state:
    st.session_state['all_target_files'] = []

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("ğŸ”‘ APIã‚­ãƒ¼ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        api_key = st.text_input("Gemini APIã‚­ãƒ¼", type="password", help="Google AI Studioã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã¨è¨˜æ†¶ã‚’å…¨ã‚¯ãƒªã‚¢"):
        st.session_state['history'] = []
        st.session_state['processed_urls'] = set()
        st.session_state['is_running'] = False
        st.session_state['all_target_files'] = []
        st.rerun()

    if api_key:
        genai.configure(api_key=api_key)
    st.info("â€»APIã‚­ãƒ¼ãŒãªã„å ´åˆã€å‹•ä½œã—ã¾ã›ã‚“ã€‚")

# ==========================================
# ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°ç¾¤
# ==========================================

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼šæ—¥æœ¬æ™‚é–“å–å¾— ---
def get_jst_now_str():
    """ç¾åœ¨æ™‚åˆ»ã‚’æ—¥æœ¬æ™‚é–“(JST)ã®æ–‡å­—åˆ—ã§è¿”ã™"""
    jst = datetime.timezone(datetime.timedelta(hours=9))
    return datetime.datetime.now(jst).strftime("%Y-%m-%d %H:%M:%S")

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼šJSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•° ---
def clean_json_response(text):
    text = text.strip()
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    text = text.strip()
    match = re.search(r'\[.*\]', text, re.DOTALL)
    if match:
        return match.group(0)
    return text

# --- Excelå¼·åŠ›èª­ã¿å–ã‚Šé–¢æ•° (Pythonãƒ­ã‚¸ãƒƒã‚¯) ---
def read_excel_robust(file_path):
    extracted_data = []
    try:
        xls = pd.ExcelFile(file_path)
        for sheet_name in xls.sheet_names:
            try:
                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            except Exception:
                continue
            
            target_row_idx = -1
            col_mapping = {} 
            for r_idx, row in df.iterrows():
                row_str = row.astype(str).values
                if any("å»ƒæ£„ç‰©ã®ç¨®é¡" in s for s in row_str) or any("ç”£æ¥­å»ƒæ£„ç‰©ã®ç¨®é¡" in s for s in row_str):
                    target_row_idx = r_idx
                    for c_idx, cell_val in enumerate(row_str):
                        val = str(cell_val).replace("\n", "").replace(" ", "")
                        if "ç¨®é¡" in val:
                            col_mapping["kind"] = c_idx
                        elif "å…¨å‡¦ç†å§”è¨—é‡" in val or "å§”è¨—é‡" in val:
                            col_mapping["amount"] = c_idx
                    break 
            
            if target_row_idx != -1 and "kind" in col_mapping and "amount" in col_mapping:
                start_row = target_row_idx + 1
                for i in range(start_row, len(df)):
                    if col_mapping["kind"] >= len(df.columns) or col_mapping["amount"] >= len(df.columns):
                        continue
                    kind_val = df.iloc[i, col_mapping["kind"]]
                    amount_val = df.iloc[i, col_mapping["amount"]]
                    if pd.notna(kind_val) and pd.notna(amount_val):
                        try:
                            amt_str = str(amount_val).replace(",", "").strip()
                            amt = float(amt_str)
                            waste_type = str(kind_val).strip()
                            if "åˆè¨ˆ" in waste_type or waste_type == "" or waste_type == "nan":
                                continue
                            extracted_data.append({
                                "æå‡ºæ—¥": "", "å¯¾è±¡å¹´åº¦": "", "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸", "æ’å‡ºäº‹æ¥­è€…å": "",
                                "äº‹æ¥­ã®ç¨®é¡": "", "äº‹æ¥­å ´å": "", "ä½æ‰€": "", "è‡ªæ²»ä½“å": "",
                                "å»ƒæ£„ç‰©ã®ç¨®é¡": waste_type, "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": amt, "å‚™è€ƒ": ""
                            })
                        except ValueError:
                            continue 
    except Exception:
        return []
    return extracted_data

# --- å…±é€šé–¢æ•°ï¼šãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»å®Œå…¨ç‰ˆï¼‰ ---
def extract_data_with_ai(file_path, filename):
    file_ext = os.path.splitext(filename)[1].lower()
    
    # ã€ä¿®æ­£ã€‘å‚™è€ƒæ¬„ã®ã€ŒAIæŠ½å‡ºã€ã‚’å‰Šé™¤ã—ã€ç©ºæ¬„ã«ã™ã‚‹ä¾‹ã«å¤‰æ›´
    STRICT_PROMPT = """
    ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®å°‚é–€å®¶ã§ã™ã€‚è³‡æ–™ã‹ã‚‰ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    
    ã€é‡è¦è¦å‰‡ã€‘
    1. å‡ºåŠ›ã¯å¿…ãš **ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ** ã«å¾“ã£ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼åã¯çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„ã§ãã ã•ã„ã€‚
    2. ã€Œå®Ÿç¸¾ã€ã®æ•°å€¤ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ã€Œè¨ˆç”»ã€ã‚„ã€Œç›®æ¨™ã€ã®ã¿ã®å ´åˆã¯ã€ãã‚Œã‚’æŠ½å‡ºã—ã¦å‚™è€ƒã«ã€Œè¨ˆç”»å€¤ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
    3. Markdownè¨˜æ³•ï¼ˆ```jsonï¼‰ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

    ã€JSONå‡ºåŠ›ä¾‹ã€‘
    [
      {
        "æå‡ºæ—¥": "ä»¤å’Œ6å¹´6æœˆ30æ—¥",
        "å¯¾è±¡å¹´åº¦": "ä»¤å’Œ5å¹´åº¦",
        "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸",
        "æ’å‡ºäº‹æ¥­è€…å": "æœ‰é™ä¼šç¤¾ã€‡ã€‡",
        "äº‹æ¥­ã®ç¨®é¡": "å»ºè¨­æ¥­",
        "äº‹æ¥­å ´å": "ã€‡ã€‡å·¥äº‹ç¾å ´",
        "ä½æ‰€": "å¾³å³¶çœŒ...",
        "è‡ªæ²»ä½“å": "å¾³å³¶çœŒ",
        "å»ƒæ£„ç‰©ã®ç¨®é¡": "æ±šæ³¥",
        "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": 100.5,
        "â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 100.5,
        "â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "å‚™è€ƒ": ""
      }
    ]
    """

    if file_ext in [".xlsx", ".xls"]:
        data_list = read_excel_robust(file_path)
        if len(data_list) > 0:
            for item in data_list:
                item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
                if "æ’å‡ºäº‹æ¥­è€…å" in item and not item["æ’å‡ºäº‹æ¥­è€…å"]:
                    item["æ’å‡ºäº‹æ¥­è€…å"] = filename
            return data_list
        
        try:
            xls = pd.read_excel(file_path, sheet_name=None)
            text_buffer = f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n\n"
            for sheet_name, df in xls.items():
                text_buffer += f"--- Sheet: {sheet_name} ---\n"
                text_buffer += df.fillna("").to_csv(index=False)
                text_buffer += "\n\n"
            if len(text_buffer) > 30000:
                text_buffer = text_buffer[:30000]

            try:
                model = genai.GenerativeModel('gemini-2.5-flash')
                response = model.generate_content([STRICT_PROMPT, text_buffer], generation_config={"response_mime_type": "application/json"})
            except:
                model = genai.GenerativeModel('gemini-flash-latest')
                response = model.generate_content([STRICT_PROMPT, text_buffer], generation_config={"response_mime_type": "application/json"})

            json_str = clean_json_response(response.text)
            ai_data_list = json.loads(json_str)
            for item in ai_data_list:
                item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
                if "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton" not in item: item["â‘©å…¨å‡¦ç†å§”è¨—é‡_ton"] = 0
            return ai_data_list
        except Exception:
            return []

    elif file_ext == ".pdf":
        try:
            try:
                model = genai.GenerativeModel('gemini-2.5-flash')
            except:
                model = genai.GenerativeModel('gemini-flash-latest')

            sample_file = genai.upload_file(path=file_path, display_name=filename)
            timeout_counter = 0
            while sample_file.state.name == "PROCESSING":
                time.sleep(1)
                timeout_counter += 1
                sample_file = genai.get_file(sample_file.name)
                if timeout_counter > 600: return [] 
            
            if sample_file.state.name == "FAILED": return []
            
            try:
                response = model.generate_content([sample_file, STRICT_PROMPT], generation_config={"response_mime_type": "application/json"})
            except:
                time.sleep(2)
                response = model.generate_content([sample_file, STRICT_PROMPT], generation_config={"response_mime_type": "application/json"})
            
            json_str = clean_json_response(response.text)
            data_list = json.loads(json_str)
            for item in data_list:
                item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
            return data_list
        except Exception:
            return []
    else:
        return []

def convert_df_to_excel(df):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, index=False)
        with open(tmp.name, "rb") as f:
            data = f.read()
    return data

# ==========================================
# ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ==========================================
tab1, tab2 = st.tabs(["ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†æ", "ğŸŒ URLã‹ã‚‰è‡ªå‹•åé›†"])

# ------------------------------------------
# ã‚¿ãƒ–1ï¼šæ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# ------------------------------------------
with tab1:
    st.subheader("æ‰‹æŒã¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ")
    st.write("PDF ã¾ãŸã¯ Excelãƒ•ã‚¡ã‚¤ãƒ«(.xlsx, .xls) ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚")
    uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["pdf", "xlsx", "xls"], accept_multiple_files=True)
    if uploaded_files:
        st.info(f"{len(uploaded_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        if st.button("ğŸš€ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æé–‹å§‹", type="primary"):
            if not api_key:
                st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            else:
                progress_bar = st.progress(0)
                status_text = st.empty()
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "uploads")
                    os.makedirs(save_dir, exist_ok=True)
                    batch_data = []
                    status_text.text("AIã«ã‚ˆã‚‹åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
                    for i, uploaded_file in enumerate(uploaded_files):
                        file_path = os.path.join(save_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        status_text.text(f"åˆ†æä¸­ ({i+1}/{len(uploaded_files)}): {uploaded_file.name}")
                        extracted = extract_data_with_ai(file_path, uploaded_file.name)
                        if extracted:
                            batch_data.extend(extracted)
                        progress_bar.progress((i + 1) / len(uploaded_files))
                    
                    if batch_data:
                        df = pd.DataFrame(batch_data)
                        # åˆ—ã®æ•´ç†
                        column_mapping = {
                            'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                            'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                            'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                            'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                            'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                            'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                            'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                            'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                            'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                            'å‚™è€ƒ': 'å‚™è€ƒ'
                        }
                        target_cols = [c for c in column_mapping.keys() if c in df.columns]
                        df = df[target_cols].rename(columns=column_mapping)
                        
                        # ã€ä¿®æ­£ã€‘æ—¥æœ¬æ™‚é–“ã‚’ä½¿ç”¨
                        now = get_jst_now_str()
                        st.session_state['history'].append({
                            "time": now, "keyword": "æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "count": len(df), "df": df
                        })
                        st.success(f"ğŸ‰ åˆ†æå®Œäº†ï¼ {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                        time.sleep(1)
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    gc.collect()

# ------------------------------------------
# ã‚¿ãƒ–2ï¼šURLè‡ªå‹•åé›† & ãƒ¬ãƒãƒ¼ãƒˆ
# ------------------------------------------
with tab2:
    st.subheader("Webã‚µã‚¤ãƒˆã‹ã‚‰è‡ªå‹•åé›†")
    st.write("å¯¾è±¡URLã«ã‚ã‚‹ PDF ãŠã‚ˆã³ Excelãƒ•ã‚¡ã‚¤ãƒ« ã‚’è‡ªå‹•åé›†ã—ã¾ã™ã€‚")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        default_url = "https://www.pref.tokushima.lg.jp/jigyoshanokata/kurashi/recycling/7300999"
        target_url = st.text_input("å¯¾è±¡ã®URL", default_url)
    with col2:
        keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—", "")

    batch_size = st.number_input("è‡ªå‹•å‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚º", min_value=1, value=50, step=10)

    # ã€ä¿®æ­£ã€‘ãƒªãƒ³ã‚¯å–å¾—é–¢æ•°ï¼ˆå‡ºç¾é †ã‚’ä¿æŒï¼‰
    def get_file_links(target_url, keyword):
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(target_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, "html.parser")
            links = soup.find_all("a")
            
            target_urls = []
            seen_urls = set() # é‡è¤‡é˜²æ­¢ç”¨ã‚»ãƒƒãƒˆ
            
            for link in links:
                href = link.get("href")
                if href:
                    href_lower = href.lower()
                    if href_lower.endswith(".pdf") or href_lower.endswith(".xlsx") or href_lower.endswith(".xls"):
                        full_url = urllib.parse.urljoin(target_url, href)
                        filename = os.path.basename(urllib.parse.urlparse(full_url).path)
                        try: filename = urllib.parse.unquote(filename)
                        except: pass
                        
                        if not keyword or keyword in filename:
                            # é †åºã‚’ä¿æŒã—ã¤ã¤é‡è¤‡æ’é™¤
                            if full_url not in seen_urls:
                                target_urls.append((filename, full_url))
                                seen_urls.add(full_url)
                                
            return target_urls
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    if target_url:
        all_file_links = get_file_links(target_url, keyword)
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜ï¼ˆé †åºä¿æŒï¼‰
        st.session_state['all_target_files'] = [f[0] for f in all_file_links]

        processed_set = st.session_state['processed_urls']
        unprocessed_links = [link for link in all_file_links if link[1] not in processed_set]
        remaining_count = len(unprocessed_links)
        
        st.caption(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ç·æ•°: {len(all_file_links)}ä»¶ / å®Œäº†: {len(all_file_links)-remaining_count}ä»¶ / æ®‹ã‚Š: {remaining_count}ä»¶")

        if remaining_count > 0:
            if not st.session_state['is_running']:
                if st.button("ğŸš€ URLã‹ã‚‰ã®è‡ªå‹•å®Ÿè¡Œã‚’é–‹å§‹", type="primary"):
                    st.session_state['is_running'] = True
                    st.rerun()
        
        if st.session_state['is_running']:
            status_box = st.empty()
            batch_progress = st.progress(0)
            
            while remaining_count > 0:
                if not st.session_state['is_running']: break
                next_batch = unprocessed_links[:int(batch_size)]
                status_box.info(f"ğŸ”„ è‡ªå‹•å‡¦ç†ä¸­... æ®‹ã‚Š {remaining_count} ä»¶")
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "downloads")
                    os.makedirs(save_dir, exist_ok=True)
                    downloaded_files = []
                    headers = {"User-Agent": "Mozilla/5.0"}
                    
                    for i, (fname, furl) in enumerate(next_batch):
                        try:
                            res = requests.get(furl, headers=headers, timeout=10)
                            fpath = os.path.join(save_dir, fname)
                            with open(fpath, "wb") as f: f.write(res.content)
                            downloaded_files.append(fpath)
                            st.session_state['processed_urls'].add(furl)
                        except: pass
                        batch_progress.progress((i + 1) / len(next_batch) * 0.5)
                    
                    if downloaded_files:
                        batch_data = []
                        for i, fpath in enumerate(downloaded_files):
                            fname = os.path.basename(fpath)
                            extracted = extract_data_with_ai(fpath, fname)
                            if extracted: batch_data.extend(extracted)
                            batch_progress.progress(0.5 + (i + 1) / len(downloaded_files) * 0.5)
                        
                        if batch_data:
                            df = pd.DataFrame(batch_data)
                            # åˆ—æ•´ç†
                            column_mapping = {
                                'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                                'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                                'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                                'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                                'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                                'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                                'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                                'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                                'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                                'å‚™è€ƒ': 'å‚™è€ƒ'
                            }
                            target_cols = [c for c in column_mapping.keys() if c in df.columns]
                            df = df[target_cols].rename(columns=column_mapping)
                            
                            # ã€ä¿®æ­£ã€‘æ—¥æœ¬æ™‚é–“ã‚’ä½¿ç”¨
                            now = get_jst_now_str()
                            st.session_state['history'].append({
                                "time": now, "keyword": keyword, "count": len(df), "df": df
                            })
                
                del downloaded_files
                gc.collect()
                unprocessed_links = [link for link in all_file_links if link[1] not in st.session_state['processed_urls']]
                remaining_count = len(unprocessed_links)
                
                if remaining_count == 0:
                    st.session_state['is_running'] = False
                    status_box.success("å®Œäº†ï¼")
                    st.rerun()
                else:
                    time.sleep(1)

            if st.button("ğŸ›‘ ä¸­æ–­"):
                st.session_state['is_running'] = False
                st.rerun()

# --- å…±é€šï¼šå®Ÿè¡Œå±¥æ­´ & ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚¨ãƒªã‚¢ ---
st.markdown("---")
st.subheader("ğŸ“‚ å®Ÿè¡Œå±¥æ­´ & çµ±åˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

if len(st.session_state['history']) > 0:
    all_dfs = [item['df'] for item in st.session_state['history']]
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    # -----------------------------------------------------
    # ã€æ©Ÿèƒ½ä¿®æ­£ã€‘å–å¾—çŠ¶æ³ã®ãƒ¬ãƒãƒ¼ãƒˆï¼ˆGap Analysisï¼‰
    # -----------------------------------------------------
    st.subheader("ğŸ“Š å–å¾—çŠ¶æ³ã®ãƒ¬ãƒãƒ¼ãƒˆ")
    
    if st.session_state['all_target_files']:
        # 1. Webä¸Šã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆï¼ˆä¿å­˜ã•ã‚ŒãŸå‡ºç¾é †ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
        # â€» setã‚’ä½¿ã‚ãšã€ãƒªã‚¹ãƒˆã®é †åºã‚’ãã®ã¾ã¾ä½¿ã†
        all_targets_ordered = st.session_state['all_target_files']
        
        # 2. æŠ½å‡ºã§ããŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰
        extracted_files = set(merged_df['ãƒ•ã‚¡ã‚¤ãƒ«å'].unique())
        
        # 3. ç…§åˆ
        audit_data = []
        # ã“ã“ã§å‡ºç¾é †ã«ãƒ«ãƒ¼ãƒ—ã•ã›ã‚‹ã“ã¨ã§ã€ãƒ¬ãƒãƒ¼ãƒˆã®ä¸¦ã³é †ã‚’ä¿è¨¼ã™ã‚‹
        for fname in all_targets_ordered:
            if fname in extracted_files:
                row_count = len(merged_df[merged_df['ãƒ•ã‚¡ã‚¤ãƒ«å'] == fname])
                status = "âœ… æˆåŠŸ"
                note = ""
            else:
                row_count = 0
                status = "âš ï¸ æœªå–å¾—"
                note = "è¦ç¢ºèª"
            
            audit_data.append({
                "ãƒ•ã‚¡ã‚¤ãƒ«å": fname,
                "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status,
                "æŠ½å‡ºè¡Œæ•°": row_count,
                "å‚™è€ƒ": note
            })
        
        audit_df = pd.DataFrame(audit_data)
        
        # çµ±è¨ˆ
        success_count = len(audit_df[audit_df['ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'] == "âœ… æˆåŠŸ"])
        fail_count = len(audit_df[audit_df['ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'] == "âš ï¸ æœªå–å¾—"])
        
        col_m1, col_m2 = st.columns(2)
        with col_m1:
            st.metric("å–å¾—æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«", f"{success_count} / {len(all_targets_ordered)}")
        with col_m2:
            st.metric("æœªå–å¾—ï¼ˆè¦ç¢ºèªï¼‰", f"{fail_count} ä»¶", delta=-fail_count)
            
        if fail_count > 0:
            st.error(f"æ³¨æ„: {fail_count} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒªã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.success("ç´ æ™´ã‚‰ã—ã„ï¼ã™ã¹ã¦ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            
        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆã‚½ãƒ¼ãƒˆã›ãšã«ãã®ã¾ã¾è¡¨ç¤ºï¼å‡ºç¾é †ï¼‰
        st.dataframe(audit_df, use_container_width=True)
        
    else:
        st.info("Webè‡ªå‹•åé›†ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã“ã«ãƒ¬ãƒãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    st.markdown("---")
    st.info(f"ğŸ’¡ ç¾åœ¨åˆè¨ˆ **{len(merged_df)} è¡Œ** ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚")
    
    merged_excel = convert_df_to_excel(merged_df)
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    st.download_button(
        label="ğŸ“¦ ã™ã¹ã¦ã®çµæœã‚’çµåˆã—ã¦Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=merged_excel,
        file_name=f"waste_report_TOTAL_{now_str}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key="download_total_btn",
        type="primary"
    )
    
    with st.expander("å€‹åˆ¥ã®å±¥æ­´ã‚’è¦‹ã‚‹"):
        for i, item in enumerate(reversed(st.session_state['history'])):
            st.write(f"**{item['time']}** - [{item['keyword']}] {item['count']}ä»¶")
            st.dataframe(item['df'])
else:
    st.write("å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
