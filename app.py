import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
import json
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import datetime
import gc  # ãƒ¡ãƒ¢ãƒªè§£æ”¾ç”¨

# --- ç”»é¢è¨­å®š ---
st.set_page_config(page_title="PDFä¸€æ‹¬DL & AIæŠ½å‡º", layout="wide")

st.title("ğŸ“„ PDFãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»å°å¸³ä½œæˆã‚¢ãƒ—ãƒª")
st.markdown("""
**ã€ŒURLã‹ã‚‰ã®è‡ªå‹•åé›†ã€** ã¾ãŸã¯ **ã€Œæ‰‹æŒã¡PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€** ã®ã©ã¡ã‚‰ã‹ã‚‰ã§ã‚‚ã€
AIãŒå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€1ã¤ã®Excelå°å¸³ã«ã¾ã¨ã‚ã¾ã™ã€‚
""")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'processed_urls' not in st.session_state:
    st.session_state['processed_urls'] = set()
if 'is_running' not in st.session_state:
    st.session_state['is_running'] = False

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("ğŸ”‘ APIã‚­ãƒ¼ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        api_key = st.text_input("Gemini APIã‚­ãƒ¼", type="password", help="Google AI Studioã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã¨è¨˜æ†¶ã‚’å…¨ã‚¯ãƒªã‚¢"):
        st.session_state['history'] = []
        st.session_state['processed_urls'] = set()
        st.session_state['is_running'] = False
        st.rerun()

    if api_key:
        genai.configure(api_key=api_key)
    st.info("â€»APIã‚­ãƒ¼ãŒãªã„å ´åˆã€å‹•ä½œã—ã¾ã›ã‚“ã€‚")

# --- å…±é€šé–¢æ•°ï¼šAIã«ã‚ˆã‚‹æŠ½å‡º ---
def extract_data_with_ai(pdf_path, filename):
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
    except:
        model = genai.GenerativeModel('gemini-flash-latest')

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        sample_file = genai.upload_file(path=pdf_path, display_name=filename)
        # å¾…æ©Ÿ
        timeout_counter = 0
        while sample_file.state.name == "PROCESSING":
            time.sleep(1)
            timeout_counter += 1
            sample_file = genai.get_file(sample_file.name)
            if timeout_counter > 30: 
                return []
        
        if sample_file.state.name == "FAILED":
            return []
    except Exception:
        return []

    prompt = """
    ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®å°‚é–€å®¶ã§ã™ã€‚PDFã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºãƒ»è»¢è¨˜ã—ã¦ãã ã•ã„ã€‚

    ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    è¡¨ã«ã¯ã€Œâ‘ ç¾çŠ¶ï¼ˆå‰å¹´åº¦å®Ÿç¸¾ï¼‰ã€ã¨ã€Œâ‘¡è¨ˆç”»ï¼ˆç›®æ¨™ï¼‰ã€ã®2ã¤ã®åˆ—ãŒä¸¦ã‚“ã§ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    **å¿…ãšã€Œâ‘ ç¾çŠ¶ã€ã¾ãŸã¯ã€Œã€å‰å¹´åº¦å®Ÿç¸¾ã€‘ã€ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹åˆ—ã®æ•°å€¤ã®ã¿**ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    ã€Œâ‘¡è¨ˆç”»ã€ã‚„ã€Œã€ç›®æ¨™ã€‘ã€ã®åˆ—ã®æ•°å€¤ã¯çµ¶å¯¾ã«æŠ½å‡ºã—ãªã„ã§ãã ã•ã„ã€‚

    ã€æŠ½å‡ºé …ç›®å®šç¾©ã€‘
    1. **æå‡ºæ—¥**: è¡¨ç´™ã®å³ä¸Šã«ã‚ã‚‹æ—¥ä»˜ï¼ˆä¾‹ï¼šä»¤å’Œ6å¹´5æœˆ21æ—¥ï¼‰ã€‚
    2. **å¯¾è±¡å¹´åº¦**: ã€Œâ‘ ç¾çŠ¶ã€ã‚„ã€Œå®Ÿç¸¾ã€ãŒæŒ‡ã—ã¦ã„ã‚‹å¹´åº¦ã€‚
    3. **æ–‡æ›¸ç¨®é¡**: å…¨ã¦ã€Œå ±å‘Šæ›¸ã€ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    4. **äº‹æ¥­ã®ç¨®é¡**: ã€Œäº‹æ¥­ã®ç¨®é¡ã€æ¬„ã‹ã‚‰æŠ½å‡ºã€‚
    5. **äº‹æ¥­å ´å**: ã€Œäº‹æ¥­å ´ã®åç§°ã€ã¾ãŸã¯ã€Œå·¥å ´åãƒ»äº‹æ¥­æ‰€åã€ã‚’æŠ½å‡ºã€‚
    6. **ä½æ‰€**: ã€Œäº‹æ¥­å ´ã®æ‰€åœ¨åœ°ã€ã‚’æŠ½å‡ºã€‚
    7. **å»ƒæ£„ç‰©ã®ç¨®é¡ã”ã¨ã®è¡Œä½œæˆ**: ç”£æ¥­å»ƒæ£„ç‰©ã®ç¨®é¡ã”ã¨ã«1è¡Œä½œæˆã€‚åˆè¨ˆè¡Œã¯ä¸è¦ã€‚

    ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
    JSONå½¢å¼ã®ãƒªã‚¹ãƒˆï¼ˆé…åˆ—ï¼‰ã®ã¿å‡ºåŠ›ã€‚
    
    [
      {
        "æå‡ºæ—¥": "ä»¤å’Œ6å¹´5æœˆ21æ—¥",
        "å¯¾è±¡å¹´åº¦": "ä»¤å’Œ5å¹´åº¦",
        "æ–‡æ›¸ç¨®é¡": "å ±å‘Šæ›¸",
        "æ’å‡ºäº‹æ¥­è€…å": "æ ªå¼ä¼šç¤¾ã€‡ã€‡",
        "äº‹æ¥­ã®ç¨®é¡": "ç·åˆå·¥äº‹æ¥­",
        "äº‹æ¥­å ´å": "ç¦å²¡æ”¯åº—",
        "ä½æ‰€": "ç¦å²¡å¸‚åšå¤šåŒº...",
        "å»ƒæ£„ç‰©ã®ç¨®é¡": "ãŒã‚Œãé¡",
        "â‘©å…¨å‡¦ç†å§”è¨—é‡_ton": 1299.99,
        "â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 1299.99,
        "â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton": 0,
        "è‡ªæ²»ä½“å": "æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†",
        "å‚™è€ƒ": ""
      }
    ]
    """
    
    try:
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content([sample_file, prompt], generation_config={"response_mime_type": "application/json"})
        except Exception:
            model = genai.GenerativeModel('gemini-flash-latest')
            response = model.generate_content([sample_file, prompt], generation_config={"response_mime_type": "application/json"})
        
        data_list = json.loads(response.text)
        for item in data_list:
            item['ãƒ•ã‚¡ã‚¤ãƒ«å'] = filename
            # è‡ªæ²»ä½“åãŒå–ã‚Œãªã„å ´åˆãŒã‚ã‚‹ã®ã§è£œå®Œ
            if 'è‡ªæ²»ä½“å' not in item or not item['è‡ªæ²»ä½“å']:
                 item['è‡ªæ²»ä½“å'] = "æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†"
        return data_list

    except Exception:
        return []

def convert_df_to_excel(df):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, index=False)
        with open(tmp.name, "rb") as f:
            data = f.read()
    return data

# ==========================================
# ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ‡ã‚Šæ›¿ãˆ
# ==========================================
tab1, tab2 = st.tabs(["ğŸ“‚ PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†æ", "ğŸŒ URLã‹ã‚‰è‡ªå‹•åé›†"])

# ------------------------------------------
# ã‚¿ãƒ–1ï¼šæ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
# ------------------------------------------
with tab1:
    st.subheader("æ‰‹æŒã¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ")
    st.write("ãƒ‘ã‚½ã‚³ãƒ³ã«ã‚ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰ã€‚")
    
    uploaded_files = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        st.info(f"{len(uploaded_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        if st.button("ğŸš€ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æé–‹å§‹", type="primary"):
            if not api_key:
                st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            else:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å‡¦ç†
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "uploads")
                    os.makedirs(save_dir, exist_ok=True)
                    
                    batch_data = []
                    status_text.text("AIã«ã‚ˆã‚‹åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
                    
                    for i, uploaded_file in enumerate(uploaded_files):
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
                        file_path = os.path.join(save_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        # AIè§£æ
                        status_text.text(f"åˆ†æä¸­ ({i+1}/{len(uploaded_files)}): {uploaded_file.name}")
                        extracted = extract_data_with_ai(file_path, uploaded_file.name)
                        if extracted:
                            batch_data.extend(extracted)
                        
                        progress_bar.progress((i + 1) / len(uploaded_files))
                    
                    # çµæœä¿å­˜
                    if batch_data:
                        df = pd.DataFrame(batch_data)
                        # åˆ—æ•´ç†
                        column_mapping = {
                            'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                            'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                            'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                            'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                            'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                            'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                            'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                            'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                            'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                            'å‚™è€ƒ': 'å‚™è€ƒ'
                        }
                        target_cols = [c for c in column_mapping.keys() if c in df.columns]
                        df = df[target_cols].rename(columns=column_mapping)
                        
                        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        st.session_state['history'].append({
                            "time": now,
                            "keyword": "æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                            "count": len(df),
                            "df": df
                        })
                        
                        st.success(f"ğŸ‰ åˆ†æå®Œäº†ï¼ {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚ä¸‹ã®å±¥æ­´ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
                        time.sleep(1)
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    
                    # ãƒ¡ãƒ¢ãƒªæƒé™¤
                    gc.collect()

# ------------------------------------------
# ã‚¿ãƒ–2ï¼šURLè‡ªå‹•åé›†æ©Ÿèƒ½ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
# ------------------------------------------
with tab2:
    st.subheader("Webã‚µã‚¤ãƒˆã‹ã‚‰è‡ªå‹•åé›†")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        default_url = "https://www.pref.kagoshima.jp/aq21/kurashi-kankyo/kankyo/sangyo/seibi/r6_public.html"
        target_url = st.text_input("å¯¾è±¡ã®URL", default_url)
    with col2:
        keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—", "06")

    # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
    batch_size = st.number_input("è‡ªå‹•å‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚º", min_value=1, value=50, step=10)

    # ãƒªãƒ³ã‚¯å–å¾—é–¢æ•°
    def get_pdf_links(target_url, keyword):
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(target_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.content, "html.parser")
            links = soup.find_all("a")
            target_urls = []
            for link in links:
                href = link.get("href")
                if href and href.lower().endswith(".pdf"):
                    full_url = urllib.parse.urljoin(target_url, href)
                    filename = os.path.basename(urllib.parse.urlparse(full_url).path)
                    try: filename = urllib.parse.unquote(filename)
                    except: pass
                    if not keyword or keyword in filename:
                        target_urls.append((filename, full_url))
            return list(set(target_urls))
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    if target_url:
        all_pdf_links = get_pdf_links(target_url, keyword)
        processed_set = st.session_state['processed_urls']
        unprocessed_links = [link for link in all_pdf_links if link[1] not in processed_set]
        remaining_count = len(unprocessed_links)
        
        st.caption(f"å¯¾è±¡PDFç·æ•°: {len(all_pdf_links)}ä»¶ / å®Œäº†: {len(all_pdf_links)-remaining_count}ä»¶ / æ®‹ã‚Š: {remaining_count}ä»¶")

        if remaining_count > 0:
            if not st.session_state['is_running']:
                if st.button("ğŸš€ URLã‹ã‚‰ã®è‡ªå‹•å®Ÿè¡Œã‚’é–‹å§‹", type="primary"):
                    st.session_state['is_running'] = True
                    st.rerun()
        
        # è‡ªå‹•ãƒ«ãƒ¼ãƒ—å‡¦ç†
        if st.session_state['is_running']:
            status_box = st.empty()
            batch_progress = st.progress(0)
            
            while remaining_count > 0:
                if not st.session_state['is_running']: break
                next_batch = unprocessed_links[:int(batch_size)]
                status_box.info(f"ğŸ”„ è‡ªå‹•å‡¦ç†ä¸­... æ®‹ã‚Š {remaining_count} ä»¶")
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    save_dir = os.path.join(temp_dir, "pdfs")
                    os.makedirs(save_dir, exist_ok=True)
                    downloaded_files = []
                    headers = {"User-Agent": "Mozilla/5.0"}
                    
                    for i, (fname, furl) in enumerate(next_batch):
                        try:
                            res = requests.get(furl, headers=headers, timeout=10)
                            fpath = os.path.join(save_dir, fname)
                            with open(fpath, "wb") as f: f.write(res.content)
                            downloaded_files.append(fpath)
                            st.session_state['processed_urls'].add(furl)
                        except: pass
                        batch_progress.progress((i + 1) / len(next_batch) * 0.5)
                    
                    if downloaded_files:
                        batch_data = []
                        for i, fpath in enumerate(downloaded_files):
                            fname = os.path.basename(fpath)
                            extracted = extract_data_with_ai(fpath, fname)
                            if extracted: batch_data.extend(extracted)
                            batch_progress.progress(0.5 + (i + 1) / len(downloaded_files) * 0.5)
                        
                        if batch_data:
                            df = pd.DataFrame(batch_data)
                            column_mapping = {
                                'ãƒ•ã‚¡ã‚¤ãƒ«å': 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'è‡ªæ²»ä½“å': 'è‡ªæ²»ä½“å', 'æå‡ºæ—¥': 'æå‡ºæ—¥',
                                'å¯¾è±¡å¹´åº¦': 'å¯¾è±¡å¹´åº¦', 'æ–‡æ›¸ç¨®é¡': 'ç¨®é¡', 'äº‹æ¥­ã®ç¨®é¡': 'äº‹æ¥­ã®ç¨®é¡',
                                'æ’å‡ºäº‹æ¥­è€…å': 'æ’å‡ºäº‹æ¥­è€…å', 'äº‹æ¥­å ´å': 'äº‹æ¥­å ´å', 'ä½æ‰€': 'ä½æ‰€',
                                'å»ƒæ£„ç‰©ã®ç¨®é¡': 'å»ƒæ£„ç‰©ã®ç¨®é¡',
                                'â‘©å…¨å‡¦ç†å§”è¨—é‡_ton': 'â‘©å…¨å‡¦ç†å§”è¨—é‡(t)',
                                'â‘ªå„ªè‰¯èªå®šå‡¦ç†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘ªå„ªè‰¯èªå®š(t)',
                                'â‘«å†ç”Ÿåˆ©ç”¨æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘«å†ç”Ÿåˆ©ç”¨(t)',
                                'â‘¬ç†±å›åèªå®šæ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘¬ç†±å›åèªå®š(t)',
                                'â‘­ç†±å›åèªå®šæ¥­è€…ä»¥å¤–ã®ç†±å›åã‚’è¡Œã†æ¥­è€…ã¸ã®å‡¦ç†å§”è¨—é‡_ton': 'â‘­ç†±å›åãã®ä»–(t)',
                                'å‚™è€ƒ': 'å‚™è€ƒ'
                            }
                            target_cols = [c for c in column_mapping.keys() if c in df.columns]
                            df = df[target_cols].rename(columns=column_mapping)
                            
                            now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            st.session_state['history'].append({
                                "time": now, "keyword": keyword, "count": len(df), "df": df
                            })
                
                del downloaded_files
                gc.collect()
                unprocessed_links = [link for link in all_pdf_links if link[1] not in st.session_state['processed_urls']]
                remaining_count = len(unprocessed_links)
                
                if remaining_count == 0:
                    st.session_state['is_running'] = False
                    status_box.success("å®Œäº†ï¼")
                    st.rerun()
                else:
                    time.sleep(1)

            if st.button("ğŸ›‘ ä¸­æ–­"):
                st.session_state['is_running'] = False
                st.rerun()

# --- å…±é€šï¼šå®Ÿè¡Œå±¥æ­´ã‚¨ãƒªã‚¢ ---
st.markdown("---")
st.subheader("ğŸ“‚ å®Ÿè¡Œå±¥æ­´ & çµ±åˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

if len(st.session_state['history']) > 0:
    all_dfs = [item['df'] for item in st.session_state['history']]
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    st.info(f"ğŸ’¡ URLæŠ½å‡ºåˆ†ãƒ»æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†ã‚ã‚ã›ã¦ã€ç¾åœ¨åˆè¨ˆ **{len(merged_df)} è¡Œ** ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚")
    
    merged_excel = convert_df_to_excel(merged_df)
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    st.download_button(
        label="ğŸ“¦ ã™ã¹ã¦ã®çµæœã‚’çµåˆã—ã¦Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=merged_excel,
        file_name=f"waste_report_TOTAL_{now_str}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key="download_total_btn",
        type="primary"
    )
    
    with st.expander("å€‹åˆ¥ã®å±¥æ­´ã‚’è¦‹ã‚‹"):
        for i, item in enumerate(reversed(st.session_state['history'])):
            st.write(f"**{item['time']}** - [{item['keyword']}] {item['count']}ä»¶")
            st.dataframe(item['df'])
else:
    st.write("å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
